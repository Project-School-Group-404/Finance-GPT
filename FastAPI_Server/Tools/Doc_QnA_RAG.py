# import os
# import time
# from langchain.tools import tool
# from langchain_text_splitters import RecursiveCharacterTextSplitter

# # from langchain_huggingface import HuggingFaceEndpointEmbeddings
# from langchain_huggingface import HuggingFaceEmbeddings
# import sentence_transformers
# from langchain_community.vectorstores import FAISS
# from langchain.chains import RetrievalQA
# from langchain_core.prompts import PromptTemplate

# # from langchain_core.runnables import RunnableParallel, RunnablePassthrough
# from langchain_google_genai import ChatGoogleGenerativeAI
# from unstract.llmwhisperer import LLMWhispererClientV2
# import hashlib


# # Environment setup
# from dotenv import load_dotenv

# load_dotenv()


# def get_file_hash(file_path: str) -> str:
#     """Compute MD5 hash of the file content."""
#     hasher = hashlib.md5()
#     with open(file_path, "rb") as f:
#         while chunk := f.read(8192):
#             hasher.update(chunk)
#     return hasher.hexdigest()


# embeddings_model = HuggingFaceEmbeddings(
#     model_name="sentence-transformers/static-retrieval-mrl-en-v1"
# )

# text_splitter = RecursiveCharacterTextSplitter(chunk_size=900, chunk_overlap=200)


# model = ChatGoogleGenerativeAI(
#     model="gemini-2.0-flash", temperature=0, google_api_key=os.getenv("GOOGLE_API_KEY")
# )


# # LLM Whisperer client setup
# llm_whisperer = LLMWhispererClientV2(
#     base_url="https://llmwhisperer-api.us-central.unstract.com/api/v2",
#     api_key=os.getenv("LLM_WHISPERER_API_KEY"),
# )

# prompt = PromptTemplate(
#     input_variables=["context", "question"],
#     template="""
#     You are a highly knowledgeable financial assistant. Use only the information provided in the context below to answer the user's question. Do not use prior knowledge.
#     Use all the context that is given to you the best you can and give the answer.
#     If the context does not contain the information needed, respond with:
#     "The answer is not available in the provided document."
#     Use the following context to answer the question.
#     ---
#     Context:
#     {context}
#     ---
#     Question: {question}

#     Answer:
#     """,
# )


# def pdf_to_text(file_path: str) -> str:
#     """Extract text from PDF using LLM Whisperer"""
#     result = llm_whisperer.whisper(file_path=file_path)

#     while True:
#         status = llm_whisperer.whisper_status(whisper_hash=result["whisper_hash"])
#         if status["status"] == "processed":
#             result = llm_whisperer.whisper_retrieve(whisper_hash=result["whisper_hash"])
#             return result["extraction"]["result_text"]
#         time.sleep(5)


# rag_chain = None

# vector_store = None

# CACHE_DIR = "./faiss_cache"
# os.makedirs(CACHE_DIR, exist_ok=True)


# def setup_rag_system(file_path: str):
#     """Process PDF and create FAISS vector store"""
#     global vector_store

#     file_hash = get_file_hash(file_path)
#     cache_path = os.path.join(CACHE_DIR, file_hash)

#     # Try loading cached FAISS index
#     if os.path.exists(cache_path):
#         try:
#             print(f"Loading cached FAISS index for file hash {file_hash}...")
#             vector_store = FAISS.load_local(
#                 cache_path, embeddings_model, allow_dangerous_deserialization=True
#             )
#             print("Loaded cached vector store successfully.")
#             return vector_store
#         except Exception as e:
#             print(f"Failed to load cache: {e}. Reprocessing...")

#     # Extract text from PDF
#     extracted_text = pdf_to_text(file_path)
#     print("Text Extracted....")

#     # Split text
#     chunks = text_splitter.split_text(extracted_text)
#     print("Chunks created....\n")

#     metadatas = [
#         {"source": os.path.basename(file_path), "file_hash": file_hash} for _ in chunks
#     ]

#     # Create FAISS index
#     vector_store = FAISS.from_texts(texts=chunks, embedding=embeddings_model)
#     vector_store.save_local(cache_path)
#     print(f"Saved FAISS index cache at {cache_path}")

#     return vector_store


# @tool
# def rag_qa_tool(file_path: str, query: str) -> str:
#     """ "
#     Query the processed document using the FAISS-based RAG system.

#     Args:
#         query (str): The user's question to be answered based on the processed document.

#     Returns:
#         str: The answer generated by the RAG system, or an error message if no document
#              has been processed or if an error occurs during processing.

#     Description:
#         This function checks if a document has been processed and a vector store exists.
#         It then invokes the RAG chain to retrieve relevant context and generate an answer
#         to the user's query.
#     """

#     vector_store = setup_rag_system(file_path=file_path)

#     if not vector_store:
#         return "No documents processed. Process a PDF first."
#     rag_chain = RetrievalQA.from_chain_type(
#         llm=model,
#         retriever=vector_store.as_retriever(search_kwargs={"k": 20}),
#         chain_type="stuff",
#         return_source_documents=True,
#     )

#     try:
#         return rag_chain.invoke(query)["result"]
#     except Exception as e:
#         return f"Error processing query: {str(e)}"


# # print(rag_qa_tool.invoke({
# #     "file_path": "/home/saikrishnanair/Finance-GPT/2PageNvidia.pdf",
# #     "query": "How much is the total revenue for the year 2024?"
# # }))







































import os
import time
from langchain.tools import tool
from langchain_text_splitters import RecursiveCharacterTextSplitter

# from langchain_huggingface import HuggingFaceEndpointEmbeddings
from langchain_huggingface import HuggingFaceEmbeddings
import sentence_transformers
from langchain_community.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain_core.prompts import PromptTemplate

# from langchain_core.runnables import RunnableParallel, RunnablePassthrough
from langchain_google_genai import ChatGoogleGenerativeAI
from unstract.llmwhisperer import LLMWhispererClientV2
import hashlib


# Environment setup
from dotenv import load_dotenv

load_dotenv()


def get_file_hash(file_path: str) -> str:
    """Compute MD5 hash of the file content."""
    hasher = hashlib.md5()
    with open(file_path, "rb") as f:
        while chunk := f.read(8192):
            hasher.update(chunk)
    return hasher.hexdigest()


embeddings_model = HuggingFaceEmbeddings(
    model_name="sentence-transformers/static-retrieval-mrl-en-v1"
)

text_splitter = RecursiveCharacterTextSplitter(chunk_size=900, chunk_overlap=200)


model = ChatGoogleGenerativeAI(
    model="gemini-2.0-flash", temperature=0, google_api_key=os.getenv("GOOGLE_API_KEY")
)


# LLM Whisperer client setup
llm_whisperer = LLMWhispererClientV2(
    base_url="https://llmwhisperer-api.us-central.unstract.com/api/v2",
    api_key=os.getenv("LLM_WHISPERER_API_KEY"),
)

prompt = PromptTemplate(
    input_variables=["context", "question"],
    template="""
You are a financial document analyst. Use only the information provided in the context below to answer the user's question. Do not use prior knowledge.

Guidelines:
1. Keep responses under 150 words and focused
2. Include specific numbers, dates, and financial metrics when available
3. Be precise and professional
4. If the question includes context from previous analysis, use that understanding

If the context does not contain the information needed, respond with:
"The answer is not available in the provided document."

Context:
{context}

Question: {question}

Answer:
""",
)


def pdf_to_text(file_path: str) -> str:
    """Extract text from PDF using LLM Whisperer"""
    result = llm_whisperer.whisper(file_path=file_path)

    while True:
        status = llm_whisperer.whisper_status(whisper_hash=result["whisper_hash"])
        if status["status"] == "processed":
            result = llm_whisperer.whisper_retrieve(whisper_hash=result["whisper_hash"])
            return result["extraction"]["result_text"]
        time.sleep(5)


rag_chain = None

vector_store = None

CACHE_DIR = "./faiss_cache"
os.makedirs(CACHE_DIR, exist_ok=True)


def setup_rag_system(file_path: str):
    """Process PDF and create FAISS vector store"""
    global vector_store

    file_hash = get_file_hash(file_path)
    cache_path = os.path.join(CACHE_DIR, file_hash)

    # Try loading cached FAISS index
    if os.path.exists(cache_path):
        try:
            print(f"Loading cached FAISS index for file hash {file_hash}...")
            vector_store = FAISS.load_local(
                cache_path, embeddings_model, allow_dangerous_deserialization=True
            )
            print("Loaded cached vector store successfully.")
            return vector_store
        except Exception as e:
            print(f"Failed to load cache: {e}. Reprocessing...")

    # Extract text from PDF
    extracted_text = pdf_to_text(file_path)
    print("Text Extracted....")

    # Split text
    chunks = text_splitter.split_text(extracted_text)
    print("Chunks created....\n")

    metadatas = [
        {"source": os.path.basename(file_path), "file_hash": file_hash} for _ in chunks
    ]

    # Create FAISS index
    vector_store = FAISS.from_texts(texts=chunks, embedding=embeddings_model)
    vector_store.save_local(cache_path)
    print(f"Saved FAISS index cache at {cache_path}")

    return vector_store


@tool
def rag_qa_tool(file_path: str, query: str) -> str:
    """
    Query the processed document using the FAISS-based RAG system.
    The query may contain context from previous agent interactions concatenated with the main question.

    Args:
        file_path (str): Path to the PDF file to be processed
        query (str): The user's question (may include context from previous analysis)

    Returns:
        str: The answer generated by the RAG system, or an error message if an error occurs.

    Description:
        This function processes the PDF document, creates/loads a vector store, and uses
        intelligent query parsing to handle concatenated context from previous agents.
    """

    print(f"Input query (with context): {query}")

    # Step 1: Use LLM to intelligently parse the query and context
    query_parsing_prompt = """
You are an expert financial document analyst. Your task is to analyze the given input (which may contain both a query and context from previous analysis) and extract the core question about the document.

The input may be in formats like:
- "How much is the total revenue for the year 2024?"
- "profit analysis based on: previous step identified focus on Q3 performance metrics"
- "balance sheet ratios context: looking for liquidity analysis from financial news"

Guidelines:
1. Parse the input to identify the main question and any contextual information
2. Extract the core question that needs to be answered about the document
3. Identify specific financial metrics, ratios, or analysis areas mentioned
4. Keep context in mind but focus on what specific information is being requested
5. Be concise and direct

Output only the refined question - no explanations or additional text.
"""

    try:
        from langchain_core.messages import SystemMessage, HumanMessage
        
        query_parsing_messages = [
            SystemMessage(content=query_parsing_prompt),
            HumanMessage(content=query)
        ]

        refined_query = model.invoke(query_parsing_messages).content.strip()
        print(f"Refined query: {refined_query}")

        # Step 2: Set up RAG system
        vector_store = setup_rag_system(file_path=file_path)

        if not vector_store:
            return "No documents processed. Process a PDF first."

        # Step 3: Create standard RAG chain (simpler approach)
        rag_chain = RetrievalQA.from_chain_type(
            llm=model,
            retriever=vector_store.as_retriever(search_kwargs={"k": 20}),
            chain_type="stuff",
            return_source_documents=True,
            chain_type_kwargs={"prompt": prompt}
        )

        # Step 4: Create context-aware query
        context_aware_query = f"Original Input: {query}\nRefined Question: {refined_query}\n\nPlease provide a focused answer under 150 words with specific numbers and dates when available."

        result = rag_chain.invoke({"query": context_aware_query})
        
        return result["result"]

    except Exception as e:
        return f"Error processing query: {str(e)}"


# Example usage with concatenated context
print(rag_qa_tool.invoke({
    "file_path": "/home/saikrishnanair/Downloads/2PageNvidia.pdf",
    "query": "How much is the total revenue for the year 2024?"
}))

# Basic usage
# print(rag_qa_tool.invoke({
#     "file_path": "/home/saikrishnanair/Finance-GPT/2PageNvidia.pdf",
#     "query": "How much is the total revenue for the year 2024?"
# }))